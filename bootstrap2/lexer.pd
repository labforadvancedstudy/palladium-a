// Lexer for Palladium - Tokenization
// "Breaking down legends into tokens"

// Token types
enum TokenType {
    // Literals
    Integer,
    String,
    Identifier,
    
    // Keywords
    Fn,
    Let,
    Mut,
    If,
    Else,
    While,
    For,
    Return,
    Import,
    Pub,
    Struct,
    Enum,
    True,
    False,
    
    // Types
    TypeI32,
    TypeI64,
    TypeBool,
    TypeString,
    
    // Operators
    Plus,
    Minus,
    Star,
    Slash,
    Percent,
    Eq,
    Ne,
    Lt,
    Gt,
    Le,
    Ge,
    And,
    Or,
    Not,
    Assign,
    
    // Delimiters
    LeftParen,
    RightParen,
    LeftBrace,
    RightBrace,
    LeftBracket,
    RightBracket,
    Comma,
    Semicolon,
    Colon,
    Arrow,
    Dot,
    Range,
    
    // Special
    Eof,
    Unknown,
}

struct Token {
    token_type: TokenType,
    lexeme: String,
    line: i64,
    column: i64,
}

struct Lexer {
    input: String,
    position: i64,
    line: i64,
    column: i64,
}

pub fn create_lexer(input: String) -> Lexer {
    return Lexer {
        input: input,
        position: 0,
        line: 1,
        column: 1,
    };
}

fn is_whitespace(c: i64) -> bool {
    return c == 32 || c == 9 || c == 10 || c == 13;  // space, tab, newline, carriage return
}

fn is_digit(c: i64) -> bool {
    return c >= 48 && c <= 57;  // '0' to '9'
}

fn is_alpha(c: i64) -> bool {
    return (c >= 65 && c <= 90) || (c >= 97 && c <= 122) || c == 95;  // A-Z, a-z, _
}

fn is_alphanumeric(c: i64) -> bool {
    return is_alpha(c) || is_digit(c);
}

fn peek_char(lexer: &Lexer) -> i64 {
    if lexer.position >= string_len(lexer.input) {
        return -1;  // EOF
    }
    return string_char_at(lexer.input, lexer.position);
}

fn advance_char(lexer: &mut Lexer) -> i64 {
    let c = peek_char(lexer);
    if c != -1 {
        lexer.position = lexer.position + 1;
        if c == 10 {  // newline
            lexer.line = lexer.line + 1;
            lexer.column = 1;
        } else {
            lexer.column = lexer.column + 1;
        }
    }
    return c;
}

fn skip_whitespace(lexer: &mut Lexer) {
    while is_whitespace(peek_char(lexer)) {
        advance_char(lexer);
    }
}

fn skip_line_comment(lexer: &mut Lexer) {
    while peek_char(lexer) != 10 && peek_char(lexer) != -1 {
        advance_char(lexer);
    }
}

fn read_string(lexer: &mut Lexer) -> String {
    advance_char(lexer);  // skip opening quote
    let start = lexer.position;
    
    while peek_char(lexer) != 34 && peek_char(lexer) != -1 {  // 34 is '"'
        if peek_char(lexer) == 92 {  // 92 is '\'
            advance_char(lexer);  // skip backslash
            advance_char(lexer);  // skip escaped char
        } else {
            advance_char(lexer);
        }
    }
    
    let value = string_substring(lexer.input, start, lexer.position);
    advance_char(lexer);  // skip closing quote
    return value;
}

fn read_number(lexer: &mut Lexer) -> String {
    let start = lexer.position;
    
    while is_digit(peek_char(lexer)) {
        advance_char(lexer);
    }
    
    return string_substring(lexer.input, start, lexer.position);
}

fn read_identifier(lexer: &mut Lexer) -> String {
    let start = lexer.position;
    
    while is_alphanumeric(peek_char(lexer)) {
        advance_char(lexer);
    }
    
    return string_substring(lexer.input, start, lexer.position);
}

fn get_keyword_type(word: String) -> TokenType {
    if string_eq(word, "fn") { return TokenType::Fn; }
    if string_eq(word, "let") { return TokenType::Let; }
    if string_eq(word, "mut") { return TokenType::Mut; }
    if string_eq(word, "if") { return TokenType::If; }
    if string_eq(word, "else") { return TokenType::Else; }
    if string_eq(word, "while") { return TokenType::While; }
    if string_eq(word, "for") { return TokenType::For; }
    if string_eq(word, "return") { return TokenType::Return; }
    if string_eq(word, "import") { return TokenType::Import; }
    if string_eq(word, "pub") { return TokenType::Pub; }
    if string_eq(word, "struct") { return TokenType::Struct; }
    if string_eq(word, "enum") { return TokenType::Enum; }
    if string_eq(word, "true") { return TokenType::True; }
    if string_eq(word, "false") { return TokenType::False; }
    if string_eq(word, "i32") { return TokenType::TypeI32; }
    if string_eq(word, "i64") { return TokenType::TypeI64; }
    if string_eq(word, "bool") { return TokenType::TypeBool; }
    if string_eq(word, "String") { return TokenType::TypeString; }
    return TokenType::Identifier;
}

pub fn next_token(lexer: &mut Lexer) -> Token {
    skip_whitespace(lexer);
    
    let line = lexer.line;
    let column = lexer.column;
    let c = peek_char(lexer);
    
    // EOF
    if c == -1 {
        return Token {
            token_type: TokenType::Eof,
            lexeme: "",
            line: line,
            column: column,
        };
    }
    
    // Comments
    if c == 47 {  // '/'
        advance_char(lexer);
        if peek_char(lexer) == 47 {  // second '/'
            skip_line_comment(lexer);
            return next_token(lexer);  // recursive call to get next real token
        } else {
            return Token {
                token_type: TokenType::Slash,
                lexeme: "/",
                line: line,
                column: column,
            };
        }
    }
    
    // String literals
    if c == 34 {  // '"'
        let value = read_string(lexer);
        return Token {
            token_type: TokenType::String,
            lexeme: value,
            line: line,
            column: column,
        };
    }
    
    // Numbers
    if is_digit(c) {
        let value = read_number(lexer);
        return Token {
            token_type: TokenType::Integer,
            lexeme: value,
            line: line,
            column: column,
        };
    }
    
    // Identifiers and keywords
    if is_alpha(c) {
        let word = read_identifier(lexer);
        let token_type = get_keyword_type(word);
        return Token {
            token_type: token_type,
            lexeme: word,
            line: line,
            column: column,
        };
    }
    
    // Single character tokens
    advance_char(lexer);
    
    if c == 43 { return Token { token_type: TokenType::Plus, lexeme: "+", line: line, column: column }; }
    if c == 45 { 
        // Check for ->
        if peek_char(lexer) == 62 {
            advance_char(lexer);
            return Token { token_type: TokenType::Arrow, lexeme: "->", line: line, column: column };
        }
        return Token { token_type: TokenType::Minus, lexeme: "-", line: line, column: column }; 
    }
    if c == 42 { return Token { token_type: TokenType::Star, lexeme: "*", line: line, column: column }; }
    if c == 37 { return Token { token_type: TokenType::Percent, lexeme: "%", line: line, column: column }; }
    if c == 40 { return Token { token_type: TokenType::LeftParen, lexeme: "(", line: line, column: column }; }
    if c == 41 { return Token { token_type: TokenType::RightParen, lexeme: ")", line: line, column: column }; }
    if c == 123 { return Token { token_type: TokenType::LeftBrace, lexeme: "{", line: line, column: column }; }
    if c == 125 { return Token { token_type: TokenType::RightBrace, lexeme: "}", line: line, column: column }; }
    if c == 91 { return Token { token_type: TokenType::LeftBracket, lexeme: "[", line: line, column: column }; }
    if c == 93 { return Token { token_type: TokenType::RightBracket, lexeme: "]", line: line, column: column }; }
    if c == 44 { return Token { token_type: TokenType::Comma, lexeme: ",", line: line, column: column }; }
    if c == 59 { return Token { token_type: TokenType::Semicolon, lexeme: ";", line: line, column: column }; }
    if c == 58 { return Token { token_type: TokenType::Colon, lexeme: ":", line: line, column: column }; }
    
    if c == 46 {  // '.'
        // Check for ..
        if peek_char(lexer) == 46 {
            advance_char(lexer);
            return Token { token_type: TokenType::Range, lexeme: "..", line: line, column: column };
        }
        return Token { token_type: TokenType::Dot, lexeme: ".", line: line, column: column };
    }
    
    if c == 61 {  // '='
        // Check for ==
        if peek_char(lexer) == 61 {
            advance_char(lexer);
            return Token { token_type: TokenType::Eq, lexeme: "==", line: line, column: column };
        }
        return Token { token_type: TokenType::Assign, lexeme: "=", line: line, column: column };
    }
    
    if c == 33 {  // '!'
        // Check for !=
        if peek_char(lexer) == 61 {
            advance_char(lexer);
            return Token { token_type: TokenType::Ne, lexeme: "!=", line: line, column: column };
        }
        return Token { token_type: TokenType::Not, lexeme: "!", line: line, column: column };
    }
    
    if c == 60 {  // '<'
        // Check for <=
        if peek_char(lexer) == 61 {
            advance_char(lexer);
            return Token { token_type: TokenType::Le, lexeme: "<=", line: line, column: column };
        }
        return Token { token_type: TokenType::Lt, lexeme: "<", line: line, column: column };
    }
    
    if c == 62 {  // '>'
        // Check for >=
        if peek_char(lexer) == 61 {
            advance_char(lexer);
            return Token { token_type: TokenType::Ge, lexeme: ">=", line: line, column: column };
        }
        return Token { token_type: TokenType::Gt, lexeme: ">", line: line, column: column };
    }
    
    if c == 38 {  // '&'
        // Check for &&
        if peek_char(lexer) == 38 {
            advance_char(lexer);
            return Token { token_type: TokenType::And, lexeme: "&&", line: line, column: column };
        }
    }
    
    if c == 124 {  // '|'
        // Check for ||
        if peek_char(lexer) == 124 {
            advance_char(lexer);
            return Token { token_type: TokenType::Or, lexeme: "||", line: line, column: column };
        }
    }
    
    // Unknown character
    return Token {
        token_type: TokenType::Unknown,
        lexeme: string_from_char(c),
        line: line,
        column: column,
    };
}

// Test the lexer
pub fn tokenize_all(input: String) -> [Token; 1000] {
    let mut lexer = create_lexer(input);
    let mut tokens: [Token; 1000] = [Token { token_type: TokenType::Eof, lexeme: "", line: 0, column: 0 }; 1000];
    let mut count = 0;
    
    while count < 1000 {
        let token = next_token(&mut lexer);
        tokens[count] = token;
        
        if matches!(token.token_type, TokenType::Eof) {
            break;
        }
        count = count + 1;
    }
    
    return tokens;
}