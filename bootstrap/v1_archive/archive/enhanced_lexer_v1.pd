// Enhanced Lexer v1 - More complete tokenizer
// Builds on simple lexer with better token handling

fn is_space(ch: i64) -> bool {
    return ch == 32 || ch == 9 || ch == 10 || ch == 13;
}

fn is_digit(ch: i64) -> bool {
    return ch >= 48 && ch <= 57;
}

fn is_alpha(ch: i64) -> bool {
    return (ch >= 65 && ch <= 90) || (ch >= 97 && ch <= 122);
}

fn is_alnum(ch: i64) -> bool {
    return is_alpha(ch) || is_digit(ch) || ch == 95;
}

// Enhanced tokenizer that generates token list
fn tokenize_to_file(source: String, out: i64) -> i64 {
    let mut pos = 0;
    let len = string_len(source);
    let mut token_count = 0;
    
    file_write(out, "// Token stream generated by Enhanced Lexer\n");
    file_write(out, "// Format: TOKEN_TYPE position\n\n");
    
    while pos < len {
        // Skip whitespace
        while pos < len && is_space(string_char_at(source, pos)) {
            pos = pos + 1;
        }
        
        if pos >= len {
            break;
        }
        
        let ch = string_char_at(source, pos);
        let start_pos = pos;
        
        // Identifiers and keywords
        if is_alpha(ch) || ch == 95 {
            // Scan identifier
            while pos < len && is_alnum(string_char_at(source, pos)) {
                pos = pos + 1;
            }
            
            // Determine token type based on length and first char
            let id_len = pos - start_pos;
            let first_ch = string_char_at(source, start_pos);
            
            if id_len == 2 && first_ch == 102 { // fn
                file_write(out, "FN ");
            } else if id_len == 2 && first_ch == 105 { // if
                file_write(out, "IF ");
            } else if id_len == 3 && first_ch == 108 { // let
                file_write(out, "LET ");
            } else if id_len == 3 && first_ch == 109 { // mut
                file_write(out, "MUT ");
            } else if id_len == 4 && first_ch == 109 { // main
                file_write(out, "MAIN ");
            } else if id_len == 5 && first_ch == 112 { // print
                file_write(out, "PRINT ");
            } else if id_len == 6 && first_ch == 114 { // return
                file_write(out, "RETURN ");
            } else {
                file_write(out, "IDENT ");
            }
            
            // Write position
            write_number(out, start_pos);
            file_write(out, "\n");
            token_count = token_count + 1;
        }
        // Numbers
        else if is_digit(ch) {
            while pos < len && is_digit(string_char_at(source, pos)) {
                pos = pos + 1;
            }
            
            file_write(out, "NUMBER ");
            write_number(out, start_pos);
            file_write(out, "\n");
            token_count = token_count + 1;
        }
        // Single-char tokens
        else {
            if ch == 40 { file_write(out, "LPAREN "); }
            else if ch == 41 { file_write(out, "RPAREN "); }
            else if ch == 123 { file_write(out, "LBRACE "); }
            else if ch == 125 { file_write(out, "RBRACE "); }
            else if ch == 59 { file_write(out, "SEMI "); }
            else if ch == 58 { file_write(out, "COLON "); }
            else if ch == 61 { file_write(out, "ASSIGN "); }
            else if ch == 43 { file_write(out, "PLUS "); }
            else if ch == 45 { file_write(out, "MINUS "); }
            else if ch == 42 { file_write(out, "STAR "); }
            else if ch == 47 { file_write(out, "SLASH "); }
            else if ch == 60 { file_write(out, "LT "); }
            else if ch == 62 { file_write(out, "GT "); }
            else { file_write(out, "UNKNOWN "); }
            
            write_number(out, start_pos);
            file_write(out, "\n");
            token_count = token_count + 1;
            pos = pos + 1;
        }
    }
    
    file_write(out, "\n// Total tokens: ");
    write_number(out, token_count);
    file_write(out, "\n");
    
    return token_count;
}

fn write_number(out: i64, num: i64) {
    if num == 0 {
        file_write(out, "0");
        return;
    }
    
    // Simple number writing
    if num < 10 {
        write_digit(out, num);
    } else if num < 100 {
        write_digit(out, num / 10);
        write_digit(out, num % 10);
    } else if num < 1000 {
        write_digit(out, num / 100);
        write_digit(out, (num / 10) % 10);
        write_digit(out, num % 10);
    }
}

fn write_digit(out: i64, digit: i64) {
    if digit == 0 { file_write(out, "0"); }
    else if digit == 1 { file_write(out, "1"); }
    else if digit == 2 { file_write(out, "2"); }
    else if digit == 3 { file_write(out, "3"); }
    else if digit == 4 { file_write(out, "4"); }
    else if digit == 5 { file_write(out, "5"); }
    else if digit == 6 { file_write(out, "6"); }
    else if digit == 7 { file_write(out, "7"); }
    else if digit == 8 { file_write(out, "8"); }
    else if digit == 9 { file_write(out, "9"); }
}

fn main() {
    print("Enhanced Lexer v1\n");
    print("=================\n\n");
    
    // Create test program
    let test_out = file_open("lexer_test.pd");
    file_write(test_out, "fn main() {\n");
    file_write(test_out, "    let x = 42;\n");
    file_write(test_out, "    print(x + 1);\n");
    file_write(test_out, "}\n");
    file_close(test_out);
    
    // Read first line for testing
    let test_in = file_open("lexer_test.pd");
    let source_line = file_read_line(test_in);
    file_close(test_in);
    
    print("Tokenizing: ");
    print(source_line);
    print("\n\n");
    
    // Tokenize to file
    let token_out = file_open("tokens.txt");
    let count = tokenize_to_file(source_line, token_out);
    file_close(token_out);
    
    print("Generated ");
    print_int(count);
    print(" tokens\n");
    print("Output: tokens.txt\n");
}