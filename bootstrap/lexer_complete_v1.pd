// Complete Lexer v1 - Full tokenizer for Palladium
// This lexer can tokenize all Palladium language constructs

// Token types
fn TK_EOF() -> i64 { return 0; }
fn TK_IDENT() -> i64 { return 1; }
fn TK_NUMBER() -> i64 { return 2; }
fn TK_STRING() -> i64 { return 3; }
fn TK_FN() -> i64 { return 4; }
fn TK_LET() -> i64 { return 5; }
fn TK_MUT() -> i64 { return 6; }
fn TK_IF() -> i64 { return 7; }
fn TK_ELSE() -> i64 { return 8; }
fn TK_WHILE() -> i64 { return 9; }
fn TK_FOR() -> i64 { return 10; }
fn TK_RETURN() -> i64 { return 11; }
fn TK_STRUCT() -> i64 { return 12; }
fn TK_ENUM() -> i64 { return 13; }
fn TK_TRUE() -> i64 { return 14; }
fn TK_FALSE() -> i64 { return 15; }
fn TK_LPAREN() -> i64 { return 16; }
fn TK_RPAREN() -> i64 { return 17; }
fn TK_LBRACE() -> i64 { return 18; }
fn TK_RBRACE() -> i64 { return 19; }
fn TK_LBRACKET() -> i64 { return 20; }
fn TK_RBRACKET() -> i64 { return 21; }
fn TK_SEMI() -> i64 { return 22; }
fn TK_COLON() -> i64 { return 23; }
fn TK_COMMA() -> i64 { return 24; }
fn TK_DOT() -> i64 { return 25; }
fn TK_ARROW() -> i64 { return 26; }
fn TK_ASSIGN() -> i64 { return 27; }
fn TK_PLUS() -> i64 { return 28; }
fn TK_MINUS() -> i64 { return 29; }
fn TK_STAR() -> i64 { return 30; }
fn TK_SLASH() -> i64 { return 31; }
fn TK_PERCENT() -> i64 { return 32; }
fn TK_EQ() -> i64 { return 33; }
fn TK_NE() -> i64 { return 34; }
fn TK_LT() -> i64 { return 35; }
fn TK_GT() -> i64 { return 36; }
fn TK_LE() -> i64 { return 37; }
fn TK_GE() -> i64 { return 38; }
fn TK_AND() -> i64 { return 39; }
fn TK_OR() -> i64 { return 40; }
fn TK_NOT() -> i64 { return 41; }

// Character classification
fn is_whitespace(ch: i64) -> bool {
    return ch == 32 || ch == 9 || ch == 10 || ch == 13;
}

fn is_digit(ch: i64) -> bool {
    return ch >= 48 && ch <= 57;
}

fn is_alpha(ch: i64) -> bool {
    return (ch >= 65 && ch <= 90) || (ch >= 97 && ch <= 122);
}

fn is_alnum(ch: i64) -> bool {
    return is_alpha(ch) || is_digit(ch) || ch == 95; // includes _
}

// Token structure using parallel arrays
fn create_token_storage() -> [i64; 1000] {
    return [0; 1000];
}

// Token storage - using global arrays as workaround
// In real implementation would use proper data structures

// Check if string matches at position
fn match_string(source: String, pos: i64, target: String) -> bool {
    let target_len = string_len(target);
    if pos + target_len > string_len(source) {
        return false;
    }
    
    let mut i = 0;
    while i < target_len {
        if string_char_at(source, pos + i) != string_char_at(target, i) {
            return false;
        }
        i = i + 1;
    }
    
    // Check that next char is not alphanumeric (for keywords)
    if pos + target_len < string_len(source) {
        let next_ch = string_char_at(source, pos + target_len);
        if is_alnum(next_ch) {
            return false;
        }
    }
    
    return true;
}

// Skip whitespace and return new position
fn skip_whitespace(source: String, pos: i64) -> i64 {
    let mut i = pos;
    while i < string_len(source) && is_whitespace(string_char_at(source, i)) {
        i = i + 1;
    }
    return i;
}

// Scan identifier or keyword
fn scan_identifier(source: String, pos: i64, tokens: mut [i64; 1000], token_index: i64) -> i64 {
    let start = pos;
    let mut end = pos;
    
    // First char must be alpha or _
    if is_alpha(string_char_at(source, end)) || string_char_at(source, end) == 95 {
        end = end + 1;
        
        // Rest can be alnum
        while end < string_len(source) && is_alnum(string_char_at(source, end)) {
            end = end + 1;
        }
    }
    
    // Check for keywords
    let len = end - start;
    if len == 2 && match_string(source, start, "fn") {
        store_token(tokens, token_index, TK_FN(), start);
    } else if len == 2 && match_string(source, start, "if") {
        store_token(tokens, token_index, TK_IF(), start);
    } else if len == 3 && match_string(source, start, "let") {
        store_token(tokens, token_index, TK_LET(), start);
    } else if len == 3 && match_string(source, start, "mut") {
        store_token(tokens, token_index, TK_MUT(), start);
    } else if len == 3 && match_string(source, start, "for") {
        store_token(tokens, token_index, TK_FOR(), start);
    } else if len == 4 && match_string(source, start, "else") {
        store_token(tokens, token_index, TK_ELSE(), start);
    } else if len == 4 && match_string(source, start, "true") {
        store_token(tokens, token_index, TK_TRUE(), start);
    } else if len == 4 && match_string(source, start, "enum") {
        store_token(tokens, token_index, TK_ENUM(), start);
    } else if len == 5 && match_string(source, start, "while") {
        store_token(tokens, token_index, TK_WHILE(), start);
    } else if len == 5 && match_string(source, start, "false") {
        store_token(tokens, token_index, TK_FALSE(), start);
    } else if len == 6 && match_string(source, start, "return") {
        store_token(tokens, token_index, TK_RETURN(), start);
    } else if len == 6 && match_string(source, start, "struct") {
        store_token(tokens, token_index, TK_STRUCT(), start);
    } else {
        store_token(tokens, token_index, TK_IDENT(), start);
    }
    
    return end;
}

// Scan number
fn scan_number(source: String, pos: i64, tokens: mut [i64; 1000], token_index: i64) -> i64 {
    let start = pos;
    let mut end = pos;
    
    while end < string_len(source) && is_digit(string_char_at(source, end)) {
        end = end + 1;
    }
    
    store_token(tokens, token_index, TK_NUMBER(), start);
    return end;
}

// Main lexer function
fn lex(source: String, tokens: mut [i64; 1000]) -> i64 {
    let mut pos = 0;
    let mut token_count = 0;
    let source_len = string_len(source);
    
    while pos < source_len && token_count < 500 { // Max 500 tokens
        // Skip whitespace
        pos = skip_whitespace(source, pos);
        if pos >= source_len {
            break;
        }
        
        let ch = string_char_at(source, pos);
        
        // Identifiers and keywords
        if is_alpha(ch) || ch == 95 { // _ 
            pos = scan_identifier(source, pos, tokens, token_count);
            token_count = token_count + 1;
        }
        // Numbers
        else if is_digit(ch) {
            pos = scan_number(source, pos, tokens, token_count);
            token_count = token_count + 1;
        }
        // Two-character operators
        else if ch == 45 && pos + 1 < source_len && string_char_at(source, pos + 1) == 62 { // ->
            store_token(tokens, token_count, TK_ARROW(), pos);
            token_count = token_count + 1;
            pos = pos + 2;
        }
        else if ch == 61 && pos + 1 < source_len && string_char_at(source, pos + 1) == 61 { // ==
            store_token(tokens, token_count, TK_EQ(), pos);
            token_count = token_count + 1;
            pos = pos + 2;
        }
        else if ch == 33 && pos + 1 < source_len && string_char_at(source, pos + 1) == 61 { // !=
            store_token(tokens, token_count, TK_NE(), pos);
            token_count = token_count + 1;
            pos = pos + 2;
        }
        else if ch == 60 && pos + 1 < source_len && string_char_at(source, pos + 1) == 61 { // <=
            store_token(tokens, token_count, TK_LE(), pos);
            token_count = token_count + 1;
            pos = pos + 2;
        }
        else if ch == 62 && pos + 1 < source_len && string_char_at(source, pos + 1) == 61 { // >=
            store_token(tokens, token_count, TK_GE(), pos);
            token_count = token_count + 1;
            pos = pos + 2;
        }
        else if ch == 38 && pos + 1 < source_len && string_char_at(source, pos + 1) == 38 { // &&
            store_token(tokens, token_count, TK_AND(), pos);
            token_count = token_count + 1;
            pos = pos + 2;
        }
        else if ch == 124 && pos + 1 < source_len && string_char_at(source, pos + 1) == 124 { // ||
            store_token(tokens, token_count, TK_OR(), pos);
            token_count = token_count + 1;
            pos = pos + 2;
        }
        // Single character tokens
        else if ch == 40 { // (
            store_token(tokens, token_count, TK_LPAREN(), pos);
            token_count = token_count + 1;
            pos = pos + 1;
        }
        else if ch == 41 { // )
            store_token(tokens, token_count, TK_RPAREN(), pos);
            token_count = token_count + 1;
            pos = pos + 1;
        }
        else if ch == 123 { // {
            store_token(tokens, token_count, TK_LBRACE(), pos);
            token_count = token_count + 1;
            pos = pos + 1;
        }
        else if ch == 125 { // }
            store_token(tokens, token_count, TK_RBRACE(), pos);
            token_count = token_count + 1;
            pos = pos + 1;
        }
        else if ch == 91 { // [
            store_token(tokens, token_count, TK_LBRACKET(), pos);
            token_count = token_count + 1;
            pos = pos + 1;
        }
        else if ch == 93 { // ]
            store_token(tokens, token_count, TK_RBRACKET(), pos);
            token_count = token_count + 1;
            pos = pos + 1;
        }
        else if ch == 59 { // ;
            store_token(tokens, token_count, TK_SEMI(), pos);
            token_count = token_count + 1;
            pos = pos + 1;
        }
        else if ch == 58 { // :
            store_token(tokens, token_count, TK_COLON(), pos);
            token_count = token_count + 1;
            pos = pos + 1;
        }
        else if ch == 44 { // ,
            store_token(tokens, token_count, TK_COMMA(), pos);
            token_count = token_count + 1;
            pos = pos + 1;
        }
        else if ch == 46 { // .
            store_token(tokens, token_count, TK_DOT(), pos);
            token_count = token_count + 1;
            pos = pos + 1;
        }
        else if ch == 61 { // =
            store_token(tokens, token_count, TK_ASSIGN(), pos);
            token_count = token_count + 1;
            pos = pos + 1;
        }
        else if ch == 43 { // +
            store_token(tokens, token_count, TK_PLUS(), pos);
            token_count = token_count + 1;
            pos = pos + 1;
        }
        else if ch == 45 { // -
            store_token(tokens, token_count, TK_MINUS(), pos);
            token_count = token_count + 1;
            pos = pos + 1;
        }
        else if ch == 42 { // *
            store_token(tokens, token_count, TK_STAR(), pos);
            token_count = token_count + 1;
            pos = pos + 1;
        }
        else if ch == 47 { // /
            store_token(tokens, token_count, TK_SLASH(), pos);
            token_count = token_count + 1;
            pos = pos + 1;
        }
        else if ch == 37 { // %
            store_token(tokens, token_count, TK_PERCENT(), pos);
            token_count = token_count + 1;
            pos = pos + 1;
        }
        else if ch == 60 { // <
            store_token(tokens, token_count, TK_LT(), pos);
            token_count = token_count + 1;
            pos = pos + 1;
        }
        else if ch == 62 { // >
            store_token(tokens, token_count, TK_GT(), pos);
            token_count = token_count + 1;
            pos = pos + 1;
        }
        else if ch == 33 { // !
            store_token(tokens, token_count, TK_NOT(), pos);
            token_count = token_count + 1;
            pos = pos + 1;
        }
        else {
            // Skip unknown character
            pos = pos + 1;
        }
    }
    
    // Add EOF token
    store_token(tokens, token_count, TK_EOF(), pos);
    return token_count + 1;
}

// Token type to string for debugging
fn token_type_name(token_type: i64) -> String {
    if token_type == TK_EOF() { return "EOF"; }
    else if token_type == TK_IDENT() { return "IDENT"; }
    else if token_type == TK_NUMBER() { return "NUMBER"; }
    else if token_type == TK_STRING() { return "STRING"; }
    else if token_type == TK_FN() { return "FN"; }
    else if token_type == TK_LET() { return "LET"; }
    else if token_type == TK_MUT() { return "MUT"; }
    else if token_type == TK_IF() { return "IF"; }
    else if token_type == TK_ELSE() { return "ELSE"; }
    else if token_type == TK_WHILE() { return "WHILE"; }
    else if token_type == TK_FOR() { return "FOR"; }
    else if token_type == TK_RETURN() { return "RETURN"; }
    else if token_type == TK_STRUCT() { return "STRUCT"; }
    else if token_type == TK_ENUM() { return "ENUM"; }
    else if token_type == TK_TRUE() { return "TRUE"; }
    else if token_type == TK_FALSE() { return "FALSE"; }
    else if token_type == TK_LPAREN() { return "LPAREN"; }
    else if token_type == TK_RPAREN() { return "RPAREN"; }
    else if token_type == TK_LBRACE() { return "LBRACE"; }
    else if token_type == TK_RBRACE() { return "RBRACE"; }
    else if token_type == TK_LBRACKET() { return "LBRACKET"; }
    else if token_type == TK_RBRACKET() { return "RBRACKET"; }
    else if token_type == TK_SEMI() { return "SEMI"; }
    else if token_type == TK_COLON() { return "COLON"; }
    else if token_type == TK_COMMA() { return "COMMA"; }
    else if token_type == TK_DOT() { return "DOT"; }
    else if token_type == TK_ARROW() { return "ARROW"; }
    else if token_type == TK_ASSIGN() { return "ASSIGN"; }
    else if token_type == TK_PLUS() { return "PLUS"; }
    else if token_type == TK_MINUS() { return "MINUS"; }
    else if token_type == TK_STAR() { return "STAR"; }
    else if token_type == TK_SLASH() { return "SLASH"; }
    else if token_type == TK_PERCENT() { return "PERCENT"; }
    else if token_type == TK_EQ() { return "EQ"; }
    else if token_type == TK_NE() { return "NE"; }
    else if token_type == TK_LT() { return "LT"; }
    else if token_type == TK_GT() { return "GT"; }
    else if token_type == TK_LE() { return "LE"; }
    else if token_type == TK_GE() { return "GE"; }
    else if token_type == TK_AND() { return "AND"; }
    else if token_type == TK_OR() { return "OR"; }
    else if token_type == TK_NOT() { return "NOT"; }
    else { return "UNKNOWN"; }
}

fn main() {
    print("Complete Palladium Lexer v1\n");
    print("===========================\n\n");
    
    // Create test program
    let test_out = file_open("lexer_test.pd");
    file_write(test_out, "fn main() {\n");
    file_write(test_out, "    let mut x = 42;\n");
    file_write(test_out, "    if x > 0 {\n");
    file_write(test_out, "        print(x + 1);\n");
    file_write(test_out, "    } else {\n");
    file_write(test_out, "        return false;\n");
    file_write(test_out, "    }\n");
    file_write(test_out, "}\n");
    file_close(test_out);
    
    // Read source
    let src_handle = file_open("lexer_test.pd");
    let source = file_read_line(src_handle);
    file_close(src_handle);
    
    print("Source: ");
    print(source);
    print("\n\n");
    
    // Tokenize
    let mut tokens = create_token_storage();
    let token_count = lex(source, tokens);
    
    print("Tokens found: ");
    print_int(token_count);
    print("\n\n");
    
    // Print first 20 tokens
    print("First tokens:\n");
    let mut i = 0;
    while i < 20 && i < token_count {
        let token_type = get_token_type(&tokens, i);
        let token_pos = get_token_pos(&tokens, i);
        
        print("  ");
        print(token_type_name(token_type));
        print(" at position ");
        print_int(token_pos);
        print("\n");
        
        i = i + 1;
    }
    
    print("\nLexer complete!\n");
}