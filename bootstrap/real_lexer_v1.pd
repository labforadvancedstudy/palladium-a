// Real Lexer v1 - Tokenizes Palladium source code

// Token types
fn TK_EOF() -> i64 { return 0; }
fn TK_FN() -> i64 { return 1; }
fn TK_MAIN() -> i64 { return 2; }
fn TK_PRINT() -> i64 { return 3; }
fn TK_LPAREN() -> i64 { return 4; }
fn TK_RPAREN() -> i64 { return 5; }
fn TK_LBRACE() -> i64 { return 6; }
fn TK_RBRACE() -> i64 { return 7; }
fn TK_SEMICOLON() -> i64 { return 8; }
fn TK_STRING() -> i64 { return 9; }

fn is_space(ch: i64) -> bool {
    return ch == 32 || ch == 10 || ch == 13 || ch == 9;
}

fn is_letter(ch: i64) -> bool {
    return (ch >= 65 && ch <= 90) || (ch >= 97 && ch <= 122);
}

fn tokenize(handle: i64) {
    print("Tokenizing...\n");
    
    let mut count = 0;
    let line = file_read_line(handle);
    
    let mut i = 0;
    while i < string_len(line) {
        let ch = string_char_at(line, i);
        
        // Skip spaces
        if is_space(ch) {
            i = i + 1;
            continue;
        }
        
        // Check for keywords
        if ch == 102 { // 'f'
            if i + 1 < string_len(line) && string_char_at(line, i + 1) == 110 { // 'n'
                print("Token: FN\n");
                count = count + 1;
                i = i + 2;
                continue;
            }
        }
        
        // Single char tokens
        if ch == 40 { // '('
            print("Token: LPAREN\n");
            count = count + 1;
        } else if ch == 41 { // ')'
            print("Token: RPAREN\n");
            count = count + 1;
        } else if ch == 123 { // '{'
            print("Token: LBRACE\n");
            count = count + 1;
        } else if ch == 125 { // '}'
            print("Token: RBRACE\n");
            count = count + 1;
        } else if ch == 59 { // ';'
            print("Token: SEMICOLON\n");
            count = count + 1;
        }
        
        i = i + 1;
    }
    
    print("Total tokens: ");
    print_int(count);
    print("\n");
}

fn main() {
    print("Real Palladium Lexer v1\n");
    print("=======================\n\n");
    
    // Create test file
    let out = file_open("test.pd");
    file_write(out, "fn main() { print(123); }");
    file_close(out);
    
    // Tokenize it
    let input = file_open("test.pd");
    tokenize(input);
    file_close(input);
}